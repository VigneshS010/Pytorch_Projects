{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2beca661-eea8-4c88-85f7-909493f2ebc1",
   "metadata": {},
   "source": [
    "- Epoch -> One forward and Backward pass of ALL training sample\n",
    "  >\n",
    "- batch_size -> Number of training samples in one forward & backward pas\n",
    "  >\n",
    "- number of iterations -> number of passes, each pass using [batch_size] number of samples\n",
    "\n",
    "- eg: 100 samples, batch_size=20 --> 100/20=5 iterations for 1 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e703cc7-d808-41eb-9b9d-bf198b2f4bd8",
   "metadata": {},
   "source": [
    "***The Main use of the Dataloader is to introducing the Minibatch training process***\n",
    "- It automatically splits the data into batches, \n",
    "- It shuffles the data, \n",
    "- Loads data in parallel (multi threading)\n",
    "- Handles large datasets that dont fit in RAM\n",
    ">\n",
    "So while its not mathematically required, Its a standard tool to save time of batch splitting code, Speed up training,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d08e8-73f3-492f-b294-4409b4c555e3",
   "metadata": {},
   "source": [
    "### Mini Batch Gradient Descent, Batch GD and Stochastic GD\n",
    "\n",
    "- Mini Batch gradient descent = Updates new weights (batch size) per epoch (Stable)\n",
    "- Batch Gradient descent = Updates new weight only once per epoch (Slow)\n",
    "- Stochastic Gradient descent = Updates new weight every new data enters (Fast, Unstable)\n",
    "\n",
    "\n",
    "#### Mini Batch \n",
    "**Eg:-**\n",
    "- Dataset size = 100\n",
    "- Batch size = 20\n",
    "- So 100/20 = 5 mini batches\n",
    ">\n",
    "- Take 1st 20 samples - Forward pass - loss calc - backward pass - gradients - update weights\n",
    "- Take 2nd 20 samples - Forward pass - loss calc - backward pass - gradients - update weights\n",
    "- Repeat until all 5 mini batches are done\n",
    "- 1 Epoch end, weights updates 5 times\n",
    "\n",
    "***Like this Full batch GD and SGD works***\n",
    "\n",
    "\n",
    "### OPTIMIZERS\n",
    "- SGD: plain gradient step.\n",
    "- RMSprop: scales gradients based on their recent magnitudes.\n",
    "- Adam: combines momentum + RMSprop ideas.\n",
    "\n",
    "\n",
    "\n",
    "**STEP BY STEP TRAINING PROCESS**\n",
    "- Forward pass - (on your chosen batch - 1 sample, mini batch or full batch)\n",
    "  - predict y_pred\n",
    "- Loss Calculation - (difference btwn y_pred and y_true)\n",
    "- Backward pass\n",
    "  - Compute gradients of loss wrt weights\n",
    "  - This gradient depends on how many samples you used ( batch size )\n",
    "- Optimizer step (SGD, Adam, etc)\n",
    "  - Takes the gradient and updates weights differently\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b8211-8670-42db-ab9d-3540408f5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Withoud dataloader eg\n",
    " \n",
    "data = numpy.loadtxt('wine.csv')\n",
    "# training loop\n",
    "for epoch in range(1000):\n",
    "    x, y = data\n",
    "    # foward + backward + weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27f52f-8679-46d1-b414-dd46a16e3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With dataloader eg\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1000):\n",
    "    # loop over all batches\n",
    "    for i in range(total_batches):\n",
    "        x_batch, y_batch = ...\n",
    "\n",
    "# --> use datset and dataloader to load wine.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf5da1-8e30-4ede-b47c-6d8f6fb3e849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3422012b-50c0-4a32-8b51-f45ca577c005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11fc93-d577-4809-9892-184e03e69e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c9ad7d-a3f8-482f-a5e0-e0242af21b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8270\n",
      "Epoch 2, Loss: 0.5606\n",
      "Epoch 3, Loss: 0.8279\n",
      "Epoch 4, Loss: 0.8281\n",
      "Epoch 5, Loss: 0.6240\n",
      "Epoch 6, Loss: 0.7933\n",
      "Epoch 7, Loss: 0.6436\n",
      "Epoch 8, Loss: 0.7201\n",
      "Epoch 9, Loss: 0.6077\n",
      "Epoch 10, Loss: 0.6002\n"
     ]
    }
   ],
   "source": [
    "# Pytorch with Dataloader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create dataset & dataloader\n",
    "train_dataset = TensorDataset(X_train,y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)  \n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "# num_workers -> No of parallel processing (Multi threating)\n",
    "\n",
    "\n",
    "# Define Model \n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 100)\n",
    "        self.fc2 = nn.Linear(100, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return torch.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "model = IrisNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass - prediction and loss\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y)\n",
    "\n",
    "        # backward pass - gradient\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights \n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# X_test_data, y_test_data = test_loader\n",
    "# with torch.no_grad():\n",
    "#     y_predicted = model(X_test_data)\n",
    "#     y_predicted_round = y_predicted.round()\n",
    "    # # print(y_predicted, y_predicted_round)\n",
    "    # acc = y_predicted_round.eq(y_test_data).sum() / float(y_test_data.shape[0])\n",
    "    # print()\n",
    "    # print(f'Accuracy score :{acc * 100:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "715de582-301f-47aa-b608-acaa6dfcbb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0834\n",
      "Epoch 2, Loss: 0.9552\n",
      "Epoch 3, Loss: 0.9585\n",
      "Epoch 4, Loss: 0.7964\n",
      "Epoch 5, Loss: 0.7861\n",
      "Epoch 6, Loss: 0.7293\n",
      "Epoch 7, Loss: 0.7380\n",
      "Epoch 8, Loss: 0.7130\n",
      "Epoch 9, Loss: 0.7749\n",
      "Epoch 10, Loss: 0.7118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10214/2926995568.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
      "/tmp/ipykernel_10214/2926995568.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Pytorch without dataloader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Same X_train, y_train, X_test, y_test from before\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "batch_size = 16\n",
    "n_batches = int(np.ceil(len(X_train_tensor) / batch_size))\n",
    "\n",
    "model = IrisNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Manual batching loop\n",
    "for epoch in range(10):\n",
    "    permutation = torch.randperm(X_train_tensor.size(0))\n",
    "    for i in range(n_batches):\n",
    "        idx = permutation[i*batch_size:(i+1)*batch_size]\n",
    "        batch_X, batch_y = X_train_tensor[idx], y_train_tensor[idx]\n",
    "\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e16c35fd-2a93-4296-9a26-14612368e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USING TENSORFLOW\n",
    "\n",
    "# import numpy as np\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# # Load dataset\n",
    "# iris = load_iris()\n",
    "# X, y = iris.data, iris.target\n",
    "# y = to_categorical(y, num_classes=3)  # one-hot encode\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build model\n",
    "# model = Sequential([\n",
    "#     Dense(10, activation='relu', input_shape=(4,)),\n",
    "#     Dense(3, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Training with mini-batch size 16\n",
    "# model.fit(X_train, y_train, batch_size=16, epochs=20, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6398797-31d3-4e18-9be1-27475c0e1e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
