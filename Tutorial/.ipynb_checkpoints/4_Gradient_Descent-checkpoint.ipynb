{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b728f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: w=8.0000000, loss=100.0000000, grad=20.0000000\n",
      "Epoch 2: w=6.4000001, loss=64.0000000, grad=16.0000000\n",
      "Epoch 3: w=5.1199999, loss=40.9600029, grad=12.8000002\n",
      "Epoch 4: w=4.0959997, loss=26.2143993, grad=10.2399998\n",
      "Epoch 5: w=3.2767997, loss=16.7772141, grad=8.1919994\n",
      "Epoch 6: w=2.6214397, loss=10.7374163, grad=6.5535994\n",
      "Epoch 7: w=2.0971518, loss=6.8719459, grad=5.2428794\n",
      "Epoch 8: w=1.6777214, loss=4.3980455, grad=4.1943035\n",
      "Epoch 9: w=1.3421772, loss=2.8147490, grad=3.3554428\n",
      "Epoch 10: w=1.0737417, loss=1.8014395, grad=2.6843543\n",
      "Epoch 11: w=0.8589934, loss=1.1529212, grad=2.1474833\n",
      "Epoch 12: w=0.6871947, loss=0.7378696, grad=1.7179867\n",
      "Epoch 13: w=0.5497558, loss=0.4722366, grad=1.3743894\n",
      "Epoch 14: w=0.4398046, loss=0.3022314, grad=1.0995115\n",
      "Epoch 15: w=0.3518437, loss=0.1934281, grad=0.8796092\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(10.0, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(15):\n",
    "    loss = w ** 2\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch+1}: w={w.item():.7f}, loss={loss.item():.7f}, grad={w.grad.item():.7f}')\n",
    "    w.grad.zero_()\n",
    "\n",
    "\n",
    "    # In this code, we simply set the weight ** 2 as loss\n",
    "    # So the backpropagation will try to reduce the weight to 0\n",
    "    # The learning rate controls how big a step we take on each iteration\n",
    "    # The w.grad.zero_() is important because PyTorch accumulates gradients by default\n",
    "    # So we need to zero it out before the next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513e2de",
   "metadata": {},
   "source": [
    "***Gradient Descent***\n",
    "It is an Optimization algorithm that uses the gradients to update the models parameters and tries to minimize the loss\n",
    "- new_weight = old_weight - learning_rate * gradient\n",
    ">\n",
    "***Backpropagation***\n",
    "It is an algorithm that calculates the gradient of the loss function with respect to each weight and bias in the network\n",
    "- It uses chain rule from calculus, working backwardfrom the final loss through each layer of the network.It OUTPUS is a gradient value for every parameter in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e7b03",
   "metadata": {},
   "source": [
    "### Linear Regression without Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6be8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "Epoch 1: w=1.200, loss=30.00000000, grad=-120.000\n",
      "Epoch 2: w=1.680, loss=4.80000067, grad=-48.000\n",
      "Epoch 3: w=1.872, loss=0.76800019, grad=-19.200\n",
      "Epoch 4: w=1.949, loss=0.12288000, grad=-7.680\n",
      "Epoch 5: w=1.980, loss=0.01966083, grad=-3.072\n",
      "Epoch 6: w=1.992, loss=0.00314574, grad=-1.229\n",
      "Epoch 7: w=1.997, loss=0.00050332, grad=-0.492\n",
      "Epoch 8: w=1.999, loss=0.00008053, grad=-0.197\n",
      "Epoch 9: w=1.999, loss=0.00001288, grad=-0.079\n",
      "Epoch 10: w=2.000, loss=0.00000206, grad=-0.031\n",
      "Epoch 11: w=2.000, loss=0.00000033, grad=-0.013\n",
      "Epoch 12: w=2.000, loss=0.00000005, grad=-0.005\n",
      "Epoch 13: w=2.000, loss=0.00000001, grad=-0.002\n",
      "Epoch 14: w=2.000, loss=0.00000000, grad=-0.001\n",
      "Epoch 15: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 16: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 17: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 18: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 19: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 20: w=2.000, loss=0.00000000, grad=0.000\n",
      "Prediction after training: f(5) = 10.000\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# f = w * x\n",
    "\n",
    "# f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    " \n",
    "# Model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = Mean Squared Error (MSE)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() \n",
    "\n",
    "# gradient\n",
    "\n",
    "# We need to take differentiation for the loss function which is MSE\n",
    "# MSE = (1/N) * (w*x - y)^2\n",
    "# After Differentiation - we take w*x-y as inner function and 1/N * u^2 as outer function\n",
    "# Its the chain rule take differentiation of both and multiply those results\n",
    "\n",
    "# differentiation of outer function = 2/N * u\n",
    "# differentiation of inner function = x \n",
    "\n",
    "# gradient = 2/N * (w*x - y) * x  or 1/N * 2x * (wx - y)\n",
    "\n",
    "def gradient(X, Y, Y_pred):\n",
    "    return np.dot(2*X, Y_pred-Y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = Forward pass\n",
    "    Y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, Y_pred)\n",
    "\n",
    "    # gradient\n",
    "    dw = gradient(X, Y, Y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= (learning_rate * dw)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch+1}: w={w:.3f}, loss={l:.8f}, grad={dw:.3f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
    "    \n",
    "\n",
    "# New Prediction \n",
    "n = input(\"Enter a number to make a prediction: \")\n",
    "n = float(n)\n",
    "print(w*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e05371",
   "metadata": {},
   "source": [
    "### Linear Regression (Gradients using Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6afb0286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "Epoch 1: w=0.300, loss=30.00000000, grad=-30.000\n",
      "Epoch 11: w=1.665, loss=1.16278565, grad=-5.906\n",
      "Epoch 21: w=1.934, loss=0.04506890, grad=-1.163\n",
      "Epoch 31: w=1.987, loss=0.00174685, grad=-0.229\n",
      "Epoch 41: w=1.997, loss=0.00006770, grad=-0.045\n",
      "Epoch 51: w=1.999, loss=0.00000262, grad=-0.009\n",
      "Epoch 61: w=2.000, loss=0.00000010, grad=-0.002\n",
      "Epoch 71: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 81: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 91: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "# loss = Mean Squared Error (MSE)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = Forward pass\n",
    "    Y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, Y_pred)\n",
    "\n",
    "    # gradient = backward pass\n",
    "    l.backward() # dl/dw\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}: w={w:.3f}, loss={l:.8f}, grad={w.grad:.3f}')\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b1119",
   "metadata": {},
   "source": [
    "### Linear Regression Fully on Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "125e341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: w=0.750, loss=187.50000000, grad=-75.000\n",
      "Epoch 2: w=1.388, loss=135.46875000, grad=-63.750\n",
      "Epoch 3: w=1.929, loss=97.87616730, grad=-54.188\n",
      "Epoch 4: w=2.390, loss=70.71553040, grad=-46.059\n",
      "Epoch 5: w=2.781, loss=51.09196854, grad=-39.150\n",
      "Epoch 6: w=3.114, loss=36.91394043, grad=-33.278\n",
      "Epoch 7: w=3.397, loss=26.67032433, grad=-28.286\n",
      "Epoch 8: w=3.638, loss=19.26930809, grad=-24.043\n",
      "Epoch 9: w=3.842, loss=13.92207718, grad=-20.437\n",
      "Epoch 10: w=4.016, loss=10.05869961, grad=-17.371\n",
      "Epoch 11: w=4.163, loss=7.26741409, grad=-14.766\n",
      "Epoch 12: w=4.289, loss=5.25070572, grad=-12.551\n",
      "Epoch 13: w=4.395, loss=3.79363537, grad=-10.668\n",
      "Epoch 14: w=4.486, loss=2.74090099, grad=-9.068\n",
      "Epoch 15: w=4.563, loss=1.98030066, grad=-7.708\n",
      "Epoch 16: w=4.629, loss=1.43076599, grad=-6.552\n",
      "Epoch 17: w=4.684, loss=1.03372908, grad=-5.569\n",
      "Epoch 18: w=4.732, loss=0.74686909, grad=-4.734\n",
      "Epoch 19: w=4.772, loss=0.53961229, grad=-4.023\n",
      "Epoch 20: w=4.806, loss=0.38987094, grad=-3.420\n",
      "Epoch 21: w=4.835, loss=0.28168115, grad=-2.907\n",
      "Epoch 22: w=4.860, loss=0.20351526, grad=-2.471\n",
      "Epoch 23: w=4.881, loss=0.14703988, grad=-2.100\n",
      "Epoch 24: w=4.899, loss=0.10623628, grad=-1.785\n",
      "Epoch 25: w=4.914, loss=0.07675596, grad=-1.517\n",
      "Epoch 26: w=4.927, loss=0.05545649, grad=-1.290\n",
      "Epoch 27: w=4.938, loss=0.04006719, grad=-1.096\n",
      "Epoch 28: w=4.947, loss=0.02894874, grad=-0.932\n",
      "Epoch 29: w=4.955, loss=0.02091532, grad=-0.792\n",
      "Epoch 30: w=4.962, loss=0.01511133, grad=-0.673\n",
      "Epoch 31: w=4.968, loss=0.01091800, grad=-0.572\n",
      "Epoch 32: w=4.972, loss=0.00788836, grad=-0.486\n",
      "Epoch 33: w=4.977, loss=0.00569929, grad=-0.413\n",
      "Epoch 34: w=4.980, loss=0.00411768, grad=-0.351\n",
      "Epoch 35: w=4.983, loss=0.00297501, grad=-0.299\n",
      "Epoch 36: w=4.986, loss=0.00214946, grad=-0.254\n",
      "Epoch 37: w=4.988, loss=0.00155304, grad=-0.216\n",
      "Epoch 38: w=4.990, loss=0.00112204, grad=-0.183\n",
      "Epoch 39: w=4.991, loss=0.00081064, grad=-0.156\n",
      "Epoch 40: w=4.992, loss=0.00058573, grad=-0.133\n",
      "Epoch 41: w=4.994, loss=0.00042319, grad=-0.113\n",
      "Epoch 42: w=4.995, loss=0.00030575, grad=-0.096\n",
      "Epoch 43: w=4.995, loss=0.00022092, grad=-0.081\n",
      "Epoch 44: w=4.996, loss=0.00015962, grad=-0.069\n",
      "Epoch 45: w=4.997, loss=0.00011534, grad=-0.059\n",
      "Epoch 46: w=4.997, loss=0.00008332, grad=-0.050\n",
      "Epoch 47: w=4.998, loss=0.00006021, grad=-0.043\n",
      "Epoch 48: w=4.998, loss=0.00004350, grad=-0.036\n",
      "Epoch 49: w=4.998, loss=0.00003143, grad=-0.031\n",
      "Epoch 50: w=4.999, loss=0.00002271, grad=-0.026\n",
      "Epoch 51: w=4.999, loss=0.00001641, grad=-0.022\n",
      "Epoch 52: w=4.999, loss=0.00001186, grad=-0.019\n",
      "Epoch 53: w=4.999, loss=0.00000856, grad=-0.016\n",
      "Epoch 54: w=4.999, loss=0.00000619, grad=-0.014\n",
      "Epoch 55: w=4.999, loss=0.00000447, grad=-0.012\n",
      "Epoch 56: w=4.999, loss=0.00000323, grad=-0.010\n",
      "Epoch 57: w=5.000, loss=0.00000233, grad=-0.008\n",
      "Epoch 58: w=5.000, loss=0.00000168, grad=-0.007\n",
      "Epoch 59: w=5.000, loss=0.00000122, grad=-0.006\n",
      "Epoch 60: w=5.000, loss=0.00000088, grad=-0.005\n",
      "Epoch 61: w=5.000, loss=0.00000063, grad=-0.004\n",
      "Epoch 62: w=5.000, loss=0.00000046, grad=-0.004\n",
      "Epoch 63: w=5.000, loss=0.00000033, grad=-0.003\n",
      "Epoch 64: w=5.000, loss=0.00000024, grad=-0.003\n",
      "Epoch 65: w=5.000, loss=0.00000017, grad=-0.002\n",
      "Epoch 66: w=5.000, loss=0.00000012, grad=-0.002\n",
      "Epoch 67: w=5.000, loss=0.00000009, grad=-0.002\n",
      "Epoch 68: w=5.000, loss=0.00000007, grad=-0.001\n",
      "Epoch 69: w=5.000, loss=0.00000005, grad=-0.001\n",
      "Epoch 70: w=5.000, loss=0.00000003, grad=-0.001\n",
      "Epoch 71: w=5.000, loss=0.00000003, grad=-0.001\n",
      "Epoch 72: w=5.000, loss=0.00000002, grad=-0.001\n",
      "Epoch 73: w=5.000, loss=0.00000001, grad=-0.001\n",
      "Epoch 74: w=5.000, loss=0.00000001, grad=-0.001\n",
      "Epoch 75: w=5.000, loss=0.00000001, grad=-0.000\n",
      "Epoch 76: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 77: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 78: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 79: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 80: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 81: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 82: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 83: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 84: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 85: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 86: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 87: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 88: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 89: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 90: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 91: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 92: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 93: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 94: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 95: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 96: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 97: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 98: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 99: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 100: w=5.000, loss=0.00000000, grad=-0.000\n",
      "Prediction after training: f(5) = 25.000\n"
     ]
    }
   ],
   "source": [
    "# General Pipeline\n",
    "# 1. Design Model(input and output size, forward pass)\n",
    "# 2. Construct Loss and Optimizer\n",
    "# 3. Training Loop\n",
    "#    - Forward Pass: Compute Prediction and Loss\n",
    "#    - Backward Pass: Compute Gradients\n",
    "#    - Update Weights\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([5,  10, 15, 20], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Model prediction\n",
    "def forward(x):\n",
    "    return w*x \n",
    "\n",
    "# loss = Mean Squared Error (MSE)\n",
    "def loss(y, y_pred):\n",
    "    return ((y - y_pred)**2).mean()\n",
    "\n",
    "# Gradient \n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction \n",
    "    Y_pred = forward(X)\n",
    "     \n",
    "    l = loss(Y, Y_pred)\n",
    "\n",
    "    l.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch+1}: w={w:.3f}, loss={l:.8f}, grad={w.grad:.3f}')\n",
    "    \n",
    "    w.grad.zero_()\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6bc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
