{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc48e36-7437-4c9a-80cd-7cb2e03cb69a",
   "metadata": {},
   "source": [
    "#### Transfer Learning\n",
    "\n",
    "It is a concept of training a model, instead of train the model from scratch,\n",
    "- We take already pre trained model (eg. Resnet, VGG),\n",
    "- Reuse its convolutional layers (they already learned to detect edges, textures, shapes, objects),\n",
    "- Replace the last Classification layer with one that matches your dataset classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193130aa-00f1-42e5-a66c-2c4800eb8503",
   "metadata": {},
   "source": [
    "#### Transfer Learning on CIFAR10 with ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "347213b7-64a4-4693-8e73-a5b81e1216b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using : cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(\"Using :\", device)\n",
    "\n",
    "# Transforms \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Cifar dataset\n",
    "train_dataset = datasets.CIFAR10('./cifar10', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10('./cifar10', train=False, transform=transform, download=True)\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=12)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c2ebc2-8fab-4e0e-9511-03abaf380b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vignesh/anaconda3/envs/custom/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vignesh/anaconda3/envs/custom/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load Pretrained ResNet1\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all layers (feature extraction)\n",
    "for param in model.parameters():\n",
    "    param.require_grad = False\n",
    "\n",
    "# Replace the final FC layer (ImageNet = 1000 classes -> CIFAR10 = 10 classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "# move to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91875dc5-c62c-46e0-b2fd-77441a879dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  # Only train final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "934041c0-7453-4c21-8511-0cd00a776571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [100/782], Loss: 1.0102\n",
      "Epoch [1/5], Batch [200/782], Loss: 0.8349\n",
      "Epoch [1/5], Batch [300/782], Loss: 0.7863\n",
      "Epoch [1/5], Batch [400/782], Loss: 0.8267\n",
      "Epoch [1/5], Batch [500/782], Loss: 0.6222\n",
      "Epoch [1/5], Batch [600/782], Loss: 0.5850\n",
      "Epoch [1/5], Batch [700/782], Loss: 0.4025\n",
      "Epoch [1/5] Finished → Avg Loss: 0.8170, Train Acc: 73.63%\n",
      "Epoch [2/5], Batch [100/782], Loss: 0.4149\n",
      "Epoch [2/5], Batch [200/782], Loss: 0.6785\n",
      "Epoch [2/5], Batch [300/782], Loss: 1.0982\n",
      "Epoch [2/5], Batch [400/782], Loss: 0.4547\n",
      "Epoch [2/5], Batch [500/782], Loss: 0.5846\n",
      "Epoch [2/5], Batch [600/782], Loss: 0.6912\n",
      "Epoch [2/5], Batch [700/782], Loss: 0.7310\n",
      "Epoch [2/5] Finished → Avg Loss: 0.6164, Train Acc: 78.86%\n",
      "Epoch [3/5], Batch [100/782], Loss: 0.6712\n",
      "Epoch [3/5], Batch [200/782], Loss: 0.6266\n",
      "Epoch [3/5], Batch [300/782], Loss: 0.5086\n",
      "Epoch [3/5], Batch [400/782], Loss: 0.4583\n",
      "Epoch [3/5], Batch [500/782], Loss: 0.5126\n",
      "Epoch [3/5], Batch [600/782], Loss: 0.4507\n",
      "Epoch [3/5], Batch [700/782], Loss: 0.4454\n",
      "Epoch [3/5] Finished → Avg Loss: 0.5927, Train Acc: 79.37%\n",
      "Epoch [4/5], Batch [100/782], Loss: 0.8158\n",
      "Epoch [4/5], Batch [200/782], Loss: 0.5585\n",
      "Epoch [4/5], Batch [300/782], Loss: 0.6175\n",
      "Epoch [4/5], Batch [400/782], Loss: 0.5194\n",
      "Epoch [4/5], Batch [500/782], Loss: 0.6156\n",
      "Epoch [4/5], Batch [600/782], Loss: 0.5543\n",
      "Epoch [4/5], Batch [700/782], Loss: 0.7939\n",
      "Epoch [4/5] Finished → Avg Loss: 0.5798, Train Acc: 80.09%\n",
      "Epoch [5/5], Batch [100/782], Loss: 0.5934\n",
      "Epoch [5/5], Batch [200/782], Loss: 0.5716\n",
      "Epoch [5/5], Batch [300/782], Loss: 0.4466\n",
      "Epoch [5/5], Batch [400/782], Loss: 0.6776\n",
      "Epoch [5/5], Batch [500/782], Loss: 0.6574\n",
      "Epoch [5/5], Batch [600/782], Loss: 0.7856\n",
      "Epoch [5/5], Batch [700/782], Loss: 0.7591\n",
      "Epoch [5/5] Finished → Avg Loss: 0.5676, Train Acc: 80.49%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print progress every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Finished → \"\n",
    "          f\"Avg Loss: {running_loss/len(train_loader):.4f}, Train Acc: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ed7f09-5e8d-4fea-9725-6dbcb7a7281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.52%\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777327ac-4ae6-4132-af0d-412d4d90b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24409/1643721326.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"resnet_cifar10.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model \n",
    "\n",
    "torch.save(model.state_dict(), \"resnet_cifar10.pth\")\n",
    "\n",
    "# Load (make sure model architecture is same!)\n",
    "model = models.resnet18(weights=None)   # initialize same model\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)  # replace last layer again\n",
    "model.load_state_dict(torch.load(\"resnet_cifar10.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4faf96-1d6b-4ae1-91b6-07da9c3d1812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (custom)",
   "language": "python",
   "name": "custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
