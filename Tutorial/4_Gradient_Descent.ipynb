{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b728f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: w=8.0000000, loss=100.0000000, grad=20.0000000\n",
      "Epoch 2: w=6.4000001, loss=64.0000000, grad=16.0000000\n",
      "Epoch 3: w=5.1199999, loss=40.9600029, grad=12.8000002\n",
      "Epoch 4: w=4.0959997, loss=26.2143993, grad=10.2399998\n",
      "Epoch 5: w=3.2767997, loss=16.7772141, grad=8.1919994\n",
      "Epoch 6: w=2.6214397, loss=10.7374163, grad=6.5535994\n",
      "Epoch 7: w=2.0971518, loss=6.8719459, grad=5.2428794\n",
      "Epoch 8: w=1.6777214, loss=4.3980455, grad=4.1943035\n",
      "Epoch 9: w=1.3421772, loss=2.8147490, grad=3.3554428\n",
      "Epoch 10: w=1.0737417, loss=1.8014395, grad=2.6843543\n",
      "Epoch 11: w=0.8589934, loss=1.1529212, grad=2.1474833\n",
      "Epoch 12: w=0.6871947, loss=0.7378696, grad=1.7179867\n",
      "Epoch 13: w=0.5497558, loss=0.4722366, grad=1.3743894\n",
      "Epoch 14: w=0.4398046, loss=0.3022314, grad=1.0995115\n",
      "Epoch 15: w=0.3518437, loss=0.1934281, grad=0.8796092\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(10.0, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(15):\n",
    "    loss = w ** 2\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch+1}: w={w.item():.7f}, loss={loss.item():.7f}, grad={w.grad.item():.7f}')\n",
    "    w.grad.zero_()\n",
    "\n",
    "\n",
    "    # In this code, we simply set the weight ** 2 as loss\n",
    "    # So the backpropagation will try to reduce the weight to 0\n",
    "    # The learning rate controls how big a step we take on each iteration\n",
    "    # The w.grad.zero_() is important because PyTorch accumulates gradients by default\n",
    "    # So we need to zero it out before the next iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513e2de",
   "metadata": {},
   "source": [
    "***Gradient Descent***\n",
    "It is an Optimization algorithm that uses the gradients to update the models parameters and tries to minimize the loss\n",
    "- new_weight = old_weight - learning_rate * gradient\n",
    ">\n",
    "***Backpropagation***\n",
    "It is an algorithm that calculates the gradient of the loss function with respect to each weight and bias in the network\n",
    "- It uses chain rule from calculus, working backwardfrom the final loss through each layer of the network.It OUTPUS is a gradient value for every parameter in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e7b03",
   "metadata": {},
   "source": [
    "### Linear Regression without Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6be8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "Epoch 1: w=1.200, loss=30.00000000, grad=-120.000\n",
      "Epoch 2: w=1.680, loss=4.80000067, grad=-48.000\n",
      "Epoch 3: w=1.872, loss=0.76800019, grad=-19.200\n",
      "Epoch 4: w=1.949, loss=0.12288000, grad=-7.680\n",
      "Epoch 5: w=1.980, loss=0.01966083, grad=-3.072\n",
      "Epoch 6: w=1.992, loss=0.00314574, grad=-1.229\n",
      "Epoch 7: w=1.997, loss=0.00050332, grad=-0.492\n",
      "Epoch 8: w=1.999, loss=0.00008053, grad=-0.197\n",
      "Epoch 9: w=1.999, loss=0.00001288, grad=-0.079\n",
      "Epoch 10: w=2.000, loss=0.00000206, grad=-0.031\n",
      "Epoch 11: w=2.000, loss=0.00000033, grad=-0.013\n",
      "Epoch 12: w=2.000, loss=0.00000005, grad=-0.005\n",
      "Epoch 13: w=2.000, loss=0.00000001, grad=-0.002\n",
      "Epoch 14: w=2.000, loss=0.00000000, grad=-0.001\n",
      "Epoch 15: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 16: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 17: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 18: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 19: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 20: w=2.000, loss=0.00000000, grad=0.000\n",
      "Prediction after training: f(5) = 10.000\n",
      "20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# f = w * x\n",
    "\n",
    "# f = 2 * x\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    " \n",
    "# Model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = Mean Squared Error (MSE)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean() \n",
    "\n",
    "# gradient\n",
    "\n",
    "# We need to take differentiation for the loss function which is MSE\n",
    "# MSE = (1/N) * (w*x - y)^2\n",
    "# After Differentiation - we take w*x-y as inner function and 1/N * u^2 as outer function\n",
    "# Its the chain rule take differentiation of both and multiply those results\n",
    "\n",
    "# differentiation of outer function = 2/N * u\n",
    "# differentiation of inner function = x \n",
    "\n",
    "# gradient = 2/N * (w*x - y) * x  or 1/N * 2x * (wx - y)\n",
    "\n",
    "def gradient(X, Y, Y_pred):\n",
    "    return np.dot(2*X, Y_pred-Y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = Forward pass\n",
    "    Y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, Y_pred)\n",
    "\n",
    "    # gradient\n",
    "    dw = gradient(X, Y, Y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= (learning_rate * dw)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch+1}: w={w:.3f}, loss={l:.8f}, grad={dw:.3f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n",
    "    \n",
    "\n",
    "# New Prediction \n",
    "n = input(\"Enter a number to make a prediction: \")\n",
    "n = float(n)\n",
    "print(w*n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e05371",
   "metadata": {},
   "source": [
    "### Linear Regression (Gradients using Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6afb0286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "Epoch 1: w=0.300, loss=30.00000000, grad=-30.000\n",
      "Epoch 11: w=1.665, loss=1.16278565, grad=-5.906\n",
      "Epoch 21: w=1.934, loss=0.04506890, grad=-1.163\n",
      "Epoch 31: w=1.987, loss=0.00174685, grad=-0.229\n",
      "Epoch 41: w=1.997, loss=0.00006770, grad=-0.045\n",
      "Epoch 51: w=1.999, loss=0.00000262, grad=-0.009\n",
      "Epoch 61: w=2.000, loss=0.00000010, grad=-0.002\n",
      "Epoch 71: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 81: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Epoch 91: w=2.000, loss=0.00000000, grad=-0.000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "# loss = Mean Squared Error (MSE)\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction = Forward pass\n",
    "    Y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, Y_pred)\n",
    "\n",
    "    # gradient = backward pass\n",
    "    l.backward() # dl/dw\n",
    "\n",
    "    # update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}: w={w:.3f}, loss={l:.8f}, grad={w.grad:.3f}')\n",
    "    w.grad.zero_()\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b1119",
   "metadata": {},
   "source": [
    "### Linear Regression Fully on Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "125e341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = 1.322\n",
      "Epoch 0: w=0.760, loss=159.22506714\n",
      "Epoch 500: w=4.820, loss=0.04716706\n",
      "Epoch 1000: w=4.960, loss=0.00235207\n",
      "Epoch 1500: w=4.991, loss=0.00011730\n",
      "Epoch 2000: w=4.998, loss=0.00000586\n",
      "Epoch 2500: w=5.000, loss=0.00000029\n",
      "Epoch 3000: w=5.000, loss=0.00000001\n",
      "Epoch 3500: w=5.000, loss=0.00000000\n",
      "Epoch 4000: w=5.000, loss=0.00000000\n",
      "Epoch 4500: w=5.000, loss=0.00000000\n",
      "Prediction after training: f(5) = 25.000\n"
     ]
    }
   ],
   "source": [
    "# General Pipeline\n",
    "# 1. Design Model(input and output size, forward pass)\n",
    "# 2. Construct Loss and Optimizer\n",
    "# 3. Training Loop\n",
    "#    - Forward Pass: Compute Prediction and Loss\n",
    "#    - Backward Pass: Compute Gradients\n",
    "#    - Update Weights\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[5], [10], [15], [20]], dtype=torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)  # 4, 1\n",
    "\n",
    "input_size = output_size = n_features \n",
    "\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # Define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# No need to calculate loss manually\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 5000\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)   \n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # Prediction \n",
    "    Y_pred = model(X)\n",
    "     \n",
    "    l = loss(Y, Y_pred)\n",
    "\n",
    "    l.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    if epoch % 500 == 0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'Epoch {epoch}: w={w[0][0]:.3f}, loss={l:.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4d014-dbf2-41f1-b585-4659a7319a53",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6bc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8594bde-96b6-4e89-a546-98687054c4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90be87-07f5-446b-9cd9-f58cbaf51031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
